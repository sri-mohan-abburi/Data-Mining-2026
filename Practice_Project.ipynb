{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult Dataset: Data Mining Project\n",
    "\n",
    "### Part 1: Data Acquisition and Preparation\n",
    "The Adult dataset (also known as the \"Census Income\" dataset) is a classic for predicting whether an individual earns more than $50,000 per year.\n",
    "\n",
    "Preprocessing Goals for Task 1:\n",
    "\n",
    "Handle Missing Values: Records with unknown values (labeled as \"?\") must be removed.\n",
    "\n",
    "\n",
    "Feature Selection: For Task 1, we must remove all continuous attributes and keep only categorical ones.\n",
    "\n",
    "One-Hot Encoding: This transforms categorical text into binary (0 or 1) columns, which is required for scikit-learn algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Training size: 30162\n",
      "Cleaned Testing size: 15060\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load column names from adult.names documentation\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "    \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "\n",
    "# Load data - handling the whitespace after commas\n",
    "train_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "test_url  = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(train_url, names=columns, sep=r',\\s*', engine='python')\n",
    "test_df = pd.read_csv(test_url, names=columns, sep=r',\\s*', engine='python', skiprows=1)\n",
    "\n",
    "# Step 1: Remove unknown values '?'\n",
    "train_df.replace('?', np.nan, inplace=True)\n",
    "test_df.replace('?', np.nan, inplace=True)\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# Step 2: Clean target label in test (remove trailing period)\n",
    "test_df['income'] = test_df['income'].str.rstrip('.')\n",
    "\n",
    "# Step 3: Remove continuous attributes for Task 1\n",
    "continuous_cols = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "train_task1 = train_df.drop(columns=continuous_cols)\n",
    "test_task1 = test_df.drop(columns=continuous_cols)\n",
    "\n",
    "# Step 4: One-Hot Encoding\n",
    "X_train_t1 = pd.get_dummies(train_task1.drop(\"income\", axis=1))\n",
    "X_test_t1 = pd.get_dummies(test_task1.drop(\"income\", axis=1))\n",
    "y_train_t1 = train_task1[\"income\"]\n",
    "y_test_t1 = test_task1[\"income\"]\n",
    "\n",
    "# Align columns to ensure both have same features\n",
    "X_train_t1, X_test_t1 = X_train_t1.align(X_test_t1, join='left', axis=1, fill_value=0)\n",
    "\n",
    "print(f\"Cleaned Training size: {len(X_train_t1)}\")\n",
    "print(f\"Cleaned Testing size: {len(X_test_t1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Classification Models (Decision Tree & Naive Bayes)\n",
    "\n",
    "Algorithm Development: Task 1\n",
    "In this step, we evaluate two different classification strategies on the categorical-only data:\n",
    "\n",
    "Decision Tree: Uses \"Entropy\" (Information Gain) to split the data into branches.\n",
    "\n",
    "Naive Bayes: Uses probability theory (Bayes' Theorem) assuming features are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Decision Tree Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.86      0.89      0.88     11360\n",
      "        >50K       0.63      0.56      0.60      3700\n",
      "\n",
      "    accuracy                           0.81     15060\n",
      "   macro avg       0.75      0.73      0.74     15060\n",
      "weighted avg       0.81      0.81      0.81     15060\n",
      "\n",
      "TP Rate: 0.5627\n",
      "FP Rate: 0.1059\n",
      "\n",
      "=== Naive Bayes Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.92      0.74      0.82     11360\n",
      "        >50K       0.50      0.79      0.61      3700\n",
      "\n",
      "    accuracy                           0.75     15060\n",
      "   macro avg       0.71      0.77      0.72     15060\n",
      "weighted avg       0.81      0.75      0.77     15060\n",
      "\n",
      "TP Rate: 0.7927\n",
      "FP Rate: 0.2605\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n=== {name} Classification Report ===\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    \n",
    "    # Calculate FP Rate: FP / (FP + TN)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    # cm[0,0] = TN, cm[0,1] = FP, cm[1,0] = FN, cm[1,1] = TP\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    tp_rate = tp / (tp + fn) # Same as Recall for the positive class\n",
    "    \n",
    "    print(f\"TP Rate: {tp_rate:.4f}\")\n",
    "    print(f\"FP Rate: {fp_rate:.4f}\")\n",
    "\n",
    "# Initialize and run models\n",
    "dt = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "nb = BernoulliNB()\n",
    "\n",
    "evaluate_model(dt, X_train_t1, y_train_t1, X_test_t1, y_test_t1, \"Decision Tree\")\n",
    "evaluate_model(nb, X_train_t1, y_train_t1, X_test_t1, y_test_t1, \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamining_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
